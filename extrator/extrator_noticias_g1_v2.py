#!/usr/bin/env python
# coding: utf-8

# In[1]:


# Programa para extrair informa√ß√µes sobre not√≠cias do g1 (t√≠tulo, subt√≠tulo, data, autor e descri√ß√£o)

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import pandas as pd
import json
import pika

# URL do RSS
rss_url = 'https://g1.globo.com/rss/g1/'

requisicao = requests.get(rss_url)

site = BeautifulSoup(requisicao.text, "html.parser")
#print(site.prettify())


# In[2]:


# Encontra todos os itens da p√°gina
itens = site.find_all("item")

links = []

# Busca os links de cada mat√©ria e salva numa lista
for item in itens:
    links.append(item.find('guid').text)    

#links = ['https://g1.globo.com/df/distrito-federal/noticia/2024/09/17/policia-civil-cria-forca-tarefa-para-investigar-suspeita-de-incendios-criminosos-no-df.ghtml']
# In[3]:


# Padr√µes a serem removidos:
patterns_to_remove = ['üîî','üì≤','‚úÖ','üì≥','üì±', 'LEIA TAMB√âM', 'LEIA MAIS', 'Leia mais', 'V√≠deos mais assistidos',                      'Veja os v√≠deos mais assistidos', 'Veja os v√≠deos mais recentes', 'Assista a mais not√≠cias',
                      'Siga o g1 no Instagram', 'Receba as not√≠cias no WhatsApp', 'Veja outras not√≠cias', 
                      'Assista aos v√≠deos', 'Veja o plant√£o de √∫ltimas not√≠cias', 'Veja mais not√≠cias', 'REVEJA V√çDEOS',
                      'Siga, curta ou assine', 'Ou√ßa os podcasts', 'Veja v√≠deos', 'Leia outras not√≠cias', 
                      'Veja mais', 'V√çDEOS com as not√≠cias', 'Mais assistidos do g1', 'Confira mais not√≠cias',
                      'Confira outras not√≠cias', 'Acompanhe o canal do g1', 'Clique aqui e saiba mais',\
                      'Leia mais', 'Participe do canal do g1', 'Assista aos v√≠deos mais vistos', 'Siga o g1', 'V√çDEOS',
                      'Leia tamb√©m', 'Assista:', 'Siga o canal g1', 'V√≠deos:', 'VEJA TAMB√âM', 'V√≠deos mais vistos', 
                      'V√≠deos do Leste', 'V√≠deos do Norte', 'V√≠deos do Sul', 'V√≠deos do Sudeste', 'V√≠deos do Nordeste',
                      'Videos do Centroeste', 'V√≠deos do Oeste', 'Reveja os telejornais', 'Veja os telejornais']

news_data = [] # Lista de not√≠cias

# Acessa cada link da p√°gina para buscar as informa√ß√µes
for link in links:    
    requisicao = requests.get(link)
    site = BeautifulSoup(requisicao.text, "html.parser")
    try:
        titulo_element = site.find(class_='content-head__title')
        titulo = titulo_element.text if titulo_element else ''# Busca o t√≠tulo da mat√©ria
        print('T√≠tulo:', titulo, '\n')        
        subtitulo_element = site.find(class_='content-head__subtitle')
        subtitulo = subtitulo_element.text if subtitulo_element else '' # Busca o subt√≠tulo da mat√©ria
        print('Subt√≠tulo:', subtitulo, '\n')
        
        # Entra no artigo propriamente dito
        artigo = site.find('article')
        if artigo is not None:
            # Obtem uma lista contendo todos os conte√∫dos de texto espalhados pelo artigo
            descricoes = artigo.find_all(class_='content-text__container')
            if descricoes != []: # Caso o artigo possua conte√∫do de texto
                descricao_completa = ''
                for descricao in descricoes:
                    if descricao.find(class_='content-unordered-list'): # Verifica se h√° lista de conte√∫dos 
                        if descricao.find(class_='content-unordered-list').find_all('a', href=True): # Caso sejam lista com links
                            pass
                        else: # Caso sejam listas sobre conte√∫dos
                            descricao_completa += descricao.text            
                    elif any(pattern in descricao.text for pattern in patterns_to_remove): # Remove propagandas
                        pass
                    else:
                        descricao_completa += descricao.text                
            else:
                descricao_completa = ''
            
            print('Descri√ß√£o:', descricao_completa, '\n')       
            
            # Obtem uma lista contendo todas as informa√ß√µes sobre as imagens espalhados pelo artigo
            imagens = artigo.find_all(class_='content-media-image')
            if imagens != []: # Cajo hajam imagens no artigo 
                legendas_imagens = ''
                links_imagens = ''
                for imagem in imagens:
                    legendas_imagens += imagem.get('alt') + ',' + ' '# Extrai a legenda da imagem 
                    links_imagens += imagem.get('src') + ' ,' + ' ' # Extrai o link da imagem
                    print('Legendas imagens:', legendas_imagens, '\n')
                    print('Links imagens:', links_imagens, '\n')
            else: # Cajo n√£o hajam imagens no artigo 
                legendas_imagens = ''
                links_imagens = ''
        
        # Extrai as informa√ß√µes de auto e data 
        from_element = site.find(class_='content-publication-data__from')
        autor = from_element.text if from_element else ''
        print('Autor:', autor, '\n')
        
        data_element = site.find(class_='content-publication-data__updated')
        data = data_element.text if data_element else ''
        print('Data:', data, '\n') 
        
    except AttributeError:
        descricao_completa = ''
        legendas_imagens = ''
        links_imagens = ''
        print('Abortando execu√ß√£o do extrator')
        exit()
                        
     # Adicionar os dados √† lista de not√≠cias
    news_item = {
        'T√≠tulo': titulo,
        'Subt√≠tulo': subtitulo,
        'Data e Hora': data,
        'Autor': autor,
        'Texto': descricao_completa,
        'Link' : link,
        'Legendas Imagens': legendas_imagens,
        'Links_imagens' : links_imagens
    }
    news_data.append(news_item)


# In[4]:


# Converter os dados para um DataFrame do pandas
df = pd.DataFrame(news_data)

# Remover as linhas onde as colunas 'Texto' e 'Legendas Imagens' est√£o vazias
df = df[~((df['Texto'] == '') & (df['Legendas Imagens'] == ''))]

#display(df)


# In[5]:


# Obter a data e a hora atual
data_hora_atual = datetime.now()

# Converter o DataFrame para um dicion√°rio de registros
data_json = df.to_dict(orient='records')

# Salvar os dados do DataFrame em um arquivo JSON
#output_file = 'noticias_g1_' + str(data_hora_atual).replace(' ','_').replace('.','').replace(':','') + '.json'

#ith open(output_file, 'w', encoding='utf-8') as f:
#    json.dump(data_json, f, ensure_ascii=False, indent=4)

#print(f"As informa√ß√µes foram salvas no arquivo JSON: {output_file}")


# In[ ]:


connection = pika.BlockingConnection(
    pika.ConnectionParameters(host="localhost", port=5672)
)

channel = connection.channel()

# Declarar a Dead Letter Exchange (DLX)
channel.exchange_declare(exchange='dlx_exchange', exchange_type='direct')

# Declarar a Dead Letter Queue (DLQ)
channel.queue_declare(queue='dlq')

# Vincular a DLQ √† DLX
channel.queue_bind(exchange='dlx_exchange', queue='dlq', routing_key='dlq_routing_key')

channel.exchange_declare(
    exchange='crawler_exchange',
    exchange_type='direct', 
    durable=True
)

channel.queue_declare(
    queue='noticias_queue',
    arguments={
        'x-dead-letter-exchange': 'dlx_exchange',
        'x-dead-letter-routing-key': 'dlq_routing_key',
        'x-max-length': 1000  # (opcional) Tamanho m√°ximo da fila antes de mover para DLQ
    }
)

channel.queue_bind(
    exchange='crawler_exchange',
    queue='noticias_queue',
    routing_key='noticias_g1'
)



for json_content in data_json:
    channel.basic_publish(exchange="crawler_exchange", routing_key="noticias_g1", body=json.dumps(json_content))

connection.close()
